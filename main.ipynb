{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"store_data_2023.csv\"  # Replace with your file path\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset columns: ['date', 'store_no', 'region', 'city', 'postal', 'street', 'longitude', 'latitude', 'store_area', 'location_type', 'turnover', 'population', 'competitor_count', 'footfall', 'avg_temperature', 'min_temperature', 'max_temperature', 'precipitation_mm', 'snow_depth_mm', 'wind_direction_degrees', 'wind_speed_kmh', 'peak_wind_gust_kmh', 'air_pressure_hpa']\n"
     ]
    }
   ],
   "source": [
    "# List all columns in the dataset\n",
    "print(\"Original dataset columns:\", data.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify control and predictor features\n",
    "control_features = ['date', 'store_area', 'avg_temperature', 'precipitation_mm', 'wind_speed_kmh']\n",
    "predictor_features = ['latitude', 'longitude', 'air_pressure_hpa', 'store_no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine control and predictor features\n",
    "features_to_use = control_features + predictor_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features to encode: ['region', 'location_type']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Handle categorical encoding\n",
    "categorical_features = [col for col in ['region', 'location_type'] if col in data.columns]\n",
    "print(\"Categorical features to encode:\", categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One-hot encode categorical features\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_features, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded dataset columns: ['date', 'store_no', 'city', 'postal', 'street', 'longitude', 'latitude', 'store_area', 'turnover', 'population', 'competitor_count', 'footfall', 'avg_temperature', 'min_temperature', 'max_temperature', 'precipitation_mm', 'snow_depth_mm', 'wind_direction_degrees', 'wind_speed_kmh', 'peak_wind_gust_kmh', 'air_pressure_hpa', 'region_Bayern', 'region_Berlin', 'region_Brandenburg', 'region_Bremen', 'region_Hamburg', 'region_Hessen', 'region_Mecklenburg-vorpommern', 'region_Niedersachsen', 'region_Nordrhein-westfalen', 'region_Rheinland-pfalz', 'region_Saarland', 'region_Sachsen', 'region_Sachsen-anhalt', 'region_Schleswig-holstein', 'region_Thüringen', 'location_type_Einkaufszentrum', 'location_type_Fachmarktzentrum', 'location_type_Fußgängerzone', 'location_type_Hauptgeschaeftsstrasse', 'location_type_Shopping Mall']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Log the columns in the processed dataset\n",
    "print(\"Encoded dataset columns:\", data_encoded.columns.tolist())\n",
    "\n",
    "# Ensure the target variable is included\n",
    "if 'turnover' not in data_encoded.columns:\n",
    "    raise ValueError(\"The target feature 'turnover' is not in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features used for training: ['date', 'store_area', 'avg_temperature', 'precipitation_mm', 'wind_speed_kmh', 'latitude', 'longitude', 'air_pressure_hpa', 'store_no']\n"
     ]
    }
   ],
   "source": [
    "# Extract necessary columns, excluding region and location_type after encoding\n",
    "features_to_use = [col for col in features_to_use if col in data_encoded.columns]\n",
    "print(\"Features used for training:\", features_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the final dataset for training\n",
    "data_model = data_encoded[features_to_use + ['turnover']].dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract features and target\n",
    "X = data_model.drop(columns=['turnover'])\n",
    "y = data_model['turnover']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'date' column is converted to a numerical representation\n",
    "if 'date' in X.columns:\n",
    "    X['year'] = pd.to_datetime(X['date']).dt.year\n",
    "    X['month'] = pd.to_datetime(X['date']).dt.month\n",
    "    X['day'] = pd.to_datetime(X['date']).dt.day\n",
    "    X = X.drop(columns=['date'])  # Drop the original 'date' column after transformation\n",
    "\n",
    "# Scale numeric features (excluding non-numeric columns like 'date')\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM model...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\tvalid_0's rmse: 49.8642\n",
      "[20]\tvalid_0's rmse: 47.3447\n",
      "[30]\tvalid_0's rmse: 45.8902\n",
      "[40]\tvalid_0's rmse: 44.7165\n",
      "[50]\tvalid_0's rmse: 43.8143\n",
      "[60]\tvalid_0's rmse: 43.0513\n",
      "[70]\tvalid_0's rmse: 42.3792\n",
      "[80]\tvalid_0's rmse: 41.795\n",
      "[90]\tvalid_0's rmse: 41.2816\n",
      "[100]\tvalid_0's rmse: 40.9054\n",
      "[110]\tvalid_0's rmse: 40.5416\n",
      "[120]\tvalid_0's rmse: 40.2058\n",
      "[130]\tvalid_0's rmse: 39.9669\n",
      "[140]\tvalid_0's rmse: 39.7195\n",
      "[150]\tvalid_0's rmse: 39.5245\n",
      "[160]\tvalid_0's rmse: 39.3305\n",
      "[170]\tvalid_0's rmse: 39.1501\n",
      "[180]\tvalid_0's rmse: 38.9978\n",
      "[190]\tvalid_0's rmse: 38.874\n",
      "[200]\tvalid_0's rmse: 38.7554\n",
      "[210]\tvalid_0's rmse: 38.6351\n",
      "[220]\tvalid_0's rmse: 38.547\n",
      "[230]\tvalid_0's rmse: 38.4464\n",
      "[240]\tvalid_0's rmse: 38.3426\n",
      "[250]\tvalid_0's rmse: 38.2359\n",
      "[260]\tvalid_0's rmse: 38.141\n",
      "[270]\tvalid_0's rmse: 38.0703\n",
      "[280]\tvalid_0's rmse: 37.9786\n",
      "[290]\tvalid_0's rmse: 37.9259\n",
      "[300]\tvalid_0's rmse: 37.865\n",
      "[310]\tvalid_0's rmse: 37.8293\n",
      "[320]\tvalid_0's rmse: 37.7557\n",
      "[330]\tvalid_0's rmse: 37.7152\n",
      "[340]\tvalid_0's rmse: 37.6697\n",
      "[350]\tvalid_0's rmse: 37.6193\n",
      "[360]\tvalid_0's rmse: 37.5718\n",
      "[370]\tvalid_0's rmse: 37.5362\n",
      "[380]\tvalid_0's rmse: 37.4904\n",
      "[390]\tvalid_0's rmse: 37.4536\n",
      "[400]\tvalid_0's rmse: 37.4092\n",
      "[410]\tvalid_0's rmse: 37.382\n",
      "[420]\tvalid_0's rmse: 37.3475\n",
      "[430]\tvalid_0's rmse: 37.3107\n",
      "[440]\tvalid_0's rmse: 37.269\n",
      "[450]\tvalid_0's rmse: 37.2421\n",
      "[460]\tvalid_0's rmse: 37.1851\n",
      "[470]\tvalid_0's rmse: 37.1623\n",
      "[480]\tvalid_0's rmse: 37.1261\n",
      "[490]\tvalid_0's rmse: 37.1124\n",
      "[500]\tvalid_0's rmse: 37.0857\n",
      "[510]\tvalid_0's rmse: 37.0566\n",
      "[520]\tvalid_0's rmse: 37.0333\n",
      "[530]\tvalid_0's rmse: 37.0013\n",
      "[540]\tvalid_0's rmse: 36.9748\n",
      "[550]\tvalid_0's rmse: 36.9507\n",
      "[560]\tvalid_0's rmse: 36.9294\n",
      "[570]\tvalid_0's rmse: 36.9069\n",
      "[580]\tvalid_0's rmse: 36.8831\n",
      "[590]\tvalid_0's rmse: 36.8665\n",
      "[600]\tvalid_0's rmse: 36.851\n",
      "[610]\tvalid_0's rmse: 36.8373\n",
      "[620]\tvalid_0's rmse: 36.8239\n",
      "[630]\tvalid_0's rmse: 36.8146\n",
      "[640]\tvalid_0's rmse: 36.7912\n",
      "[650]\tvalid_0's rmse: 36.761\n",
      "[660]\tvalid_0's rmse: 36.7371\n",
      "[670]\tvalid_0's rmse: 36.7218\n",
      "[680]\tvalid_0's rmse: 36.7155\n",
      "[690]\tvalid_0's rmse: 36.7014\n",
      "[700]\tvalid_0's rmse: 36.6876\n",
      "[710]\tvalid_0's rmse: 36.6782\n",
      "[720]\tvalid_0's rmse: 36.6725\n",
      "[730]\tvalid_0's rmse: 36.6519\n",
      "[740]\tvalid_0's rmse: 36.6416\n",
      "[750]\tvalid_0's rmse: 36.6258\n",
      "[760]\tvalid_0's rmse: 36.6092\n",
      "[770]\tvalid_0's rmse: 36.5996\n",
      "[780]\tvalid_0's rmse: 36.5923\n",
      "[790]\tvalid_0's rmse: 36.5777\n",
      "[800]\tvalid_0's rmse: 36.5743\n",
      "[810]\tvalid_0's rmse: 36.5662\n",
      "[820]\tvalid_0's rmse: 36.565\n",
      "[830]\tvalid_0's rmse: 36.5579\n",
      "[840]\tvalid_0's rmse: 36.5505\n",
      "[850]\tvalid_0's rmse: 36.5392\n",
      "[860]\tvalid_0's rmse: 36.5288\n",
      "[870]\tvalid_0's rmse: 36.5191\n",
      "[880]\tvalid_0's rmse: 36.5162\n",
      "[890]\tvalid_0's rmse: 36.5086\n",
      "[900]\tvalid_0's rmse: 36.5006\n",
      "[910]\tvalid_0's rmse: 36.4827\n",
      "[920]\tvalid_0's rmse: 36.4778\n",
      "[930]\tvalid_0's rmse: 36.4699\n",
      "[940]\tvalid_0's rmse: 36.4655\n",
      "[950]\tvalid_0's rmse: 36.4623\n",
      "[960]\tvalid_0's rmse: 36.4455\n",
      "[970]\tvalid_0's rmse: 36.4404\n",
      "[980]\tvalid_0's rmse: 36.4315\n",
      "[990]\tvalid_0's rmse: 36.429\n",
      "[1000]\tvalid_0's rmse: 36.4248\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's rmse: 36.4248\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import early_stopping, log_evaluation\n",
    "\n",
    "print(\"Training LightGBM model...\")\n",
    "# Train with callbacks\n",
    "lgb_model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    num_boost_round=1000,  # Set a high maximum boosting round\n",
    "    valid_sets=[lgb_test],  # Specify validation data\n",
    "    callbacks=[\n",
    "        early_stopping(stopping_rounds=50),  # Stop early if no improvement in 50 rounds\n",
    "        log_evaluation(period=10)  # Log evaluation every 10 rounds\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM (convert to sequences)\n",
    "def create_sequences(features, target, seq_length=10):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(features) - seq_length):\n",
    "        X_seq.append(features[i:i + seq_length])\n",
    "        y_seq.append(target[i + seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "seq_length = 10\n",
    "X_seq, y_seq = create_sequences(X_scaled, y.values, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sequences into training and testing sets\n",
    "X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amrit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Build LSTM model\n",
    "lstm_model = Sequential([\n",
    "    LSTM(64, input_shape=(seq_length, X_train_seq.shape[2]), return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM model...\n",
      "Epoch 1/10\n",
      "\u001b[1m2235/2235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 10ms/step - loss: 2163.6760 - mae: 34.6944 - val_loss: 1910.2665 - val_mae: 33.2803\n",
      "Epoch 2/10\n",
      "\u001b[1m2235/2235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 11ms/step - loss: 2082.2468 - mae: 34.1423 - val_loss: 1876.8640 - val_mae: 32.8744\n",
      "Epoch 3/10\n",
      "\u001b[1m2235/2235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 11ms/step - loss: 2085.3640 - mae: 33.8249 - val_loss: 1868.4172 - val_mae: 32.8016\n",
      "Epoch 4/10\n",
      "\u001b[1m2235/2235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 11ms/step - loss: 2014.5327 - mae: 33.5916 - val_loss: 1851.2665 - val_mae: 32.7417\n",
      "Epoch 5/10\n",
      "\u001b[1m2235/2235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 10ms/step - loss: 1923.2200 - mae: 33.1505 - val_loss: 1841.4216 - val_mae: 32.6193\n",
      "Epoch 6/10\n",
      "\u001b[1m2235/2235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 11ms/step - loss: 1928.1167 - mae: 33.0279 - val_loss: 1839.9766 - val_mae: 32.4562\n",
      "Epoch 7/10\n",
      "\u001b[1m2235/2235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 11ms/step - loss: 1979.2838 - mae: 33.0712 - val_loss: 1790.3820 - val_mae: 32.0536\n",
      "Epoch 8/10\n",
      "\u001b[1m2235/2235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 11ms/step - loss: 1852.1241 - mae: 32.6555 - val_loss: 1795.7578 - val_mae: 32.1800\n",
      "Epoch 9/10\n",
      "\u001b[1m2235/2235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 11ms/step - loss: 1858.4823 - mae: 32.5951 - val_loss: 1791.9409 - val_mae: 32.0551\n",
      "Epoch 10/10\n",
      "\u001b[1m2235/2235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 11ms/step - loss: 1842.6775 - mae: 32.4228 - val_loss: 1766.2216 - val_mae: 31.9035\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Training LSTM model...\")\n",
    "history = lstm_model.fit(X_train_seq, y_train_seq, validation_data=(X_test_seq, y_test_seq), \n",
    "                         epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m559/559\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "LightGBM RMSE: 36.42481195124668\n",
      "LSTM RMSE: 42.026432517119694\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Evaluate both models\n",
    "lgb_preds = lgb_model.predict(X_test)\n",
    "lstm_preds = lstm_model.predict(X_test_seq)\n",
    "\n",
    "lgb_rmse = np.sqrt(mean_squared_error(y_test, lgb_preds))\n",
    "lstm_rmse = np.sqrt(mean_squared_error(y_test_seq, lstm_preds))\n",
    "\n",
    "print(f\"LightGBM RMSE: {lgb_rmse}\")\n",
    "print(f\"LSTM RMSE: {lstm_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "# Save the models for deployment\n",
    "lgb_model_path = \"lightgbm_turnover_model.pkl\"\n",
    "joblib.dump(lgb_model, lgb_model_path)\n",
    "\n",
    "lstm_model_path = \"lstm_turnover_model.h5\"\n",
    "lstm_model.save(lstm_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "lstm_model = tf.keras.models.load_model(\"lstm_turnover_model.h5\", custom_objects={\"mse\": tf.keras.losses.MeanSquaredError()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM RMSE: 36.42481195124668\n",
      "LightGBM Accuracy: 32.71%\n",
      "LSTM RMSE: 42.026432517119694\n",
      "LSTM Accuracy: 19.21%\n"
     ]
    }
   ],
   "source": [
    "# Import necessary library\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define a tolerance for regression accuracy\n",
    "tolerance = 0.1  # Example: predictions within 10% of the true value are considered accurate\n",
    "\n",
    "# Calculate accuracy for LightGBM\n",
    "lgb_accuracy = np.mean(np.abs(lgb_preds - y_test) <= tolerance * np.abs(y_test)) * 100\n",
    "\n",
    "# Calculate accuracy for LSTM\n",
    "lstm_accuracy = np.mean(np.abs(lstm_preds - y_test_seq) <= tolerance * np.abs(y_test_seq)) * 100\n",
    "\n",
    "# Print RMSE and Accuracy for both models\n",
    "print(f\"LightGBM RMSE: {lgb_rmse}\")\n",
    "print(f\"LightGBM Accuracy: {lgb_accuracy:.2f}%\")\n",
    "print(f\"LSTM RMSE: {lstm_rmse}\")\n",
    "print(f\"LSTM Accuracy: {lstm_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
